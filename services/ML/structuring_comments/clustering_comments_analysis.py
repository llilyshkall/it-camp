import os
import docx
import fitz
import torch
import pandas as pd
import asyncio
import aiohttp
from tqdm.asyncio import tqdm as async_tqdm
import json
import subprocess
import shutil
from enum import StrEnum
from pathlib import Path
from typing import NamedTuple

import jinja2

from pptx import Presentation
from bs4 import BeautifulSoup

from langchain.schema.document import Document
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ---  –ì–õ–û–ë–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø  ---
GLOBAL_CONFIG = {
    "LOCAL_API_URL": "http://89.108.116.240:11434/api/chat",
    "LOCAL_MODEL_NAME": "qwen3:8b",
    "EMBEDDING_MODEL": "intfloat/multilingual-e5-large",
    "MAX_CONCURRENT_REQUESTS": 1,
    "RETRIEVER_TOP_K": 5,
    "REQUEST_DELAY_SECONDS": 0.5
}


# --- PDF generator ----
def filter_newlines(text: str) -> str:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –¥–ª—è LaTeX"""
    # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã –∏ —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    return ' '.join(text.replace("|", "").split()).replace('%', '\\%').replace('&', '\\&').replace("_", "\\_")


def process_summary(text: str, section_label: str) -> str:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–≤–æ–¥–∫—É, –¥–æ–±–∞–≤–ª—è—è —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã"""
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ –Ω–æ–º–µ—Ä–æ–≤ –Ω–∞ —Å—Å—ã–ª–∫–∏ (–º–æ–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –≤–∞—à —Ñ–æ—Ä–º–∞—Ç)
    import re
    # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç–∏–ø–∞ "–ò–°–¢–û–ß–ù–ò–ö 1", "–∏—Å—Ç–æ—á–Ω–∏–∫ 3" –∏ —Ç.–¥.
    pattern = r'(–ò–°–¢–û–ß–ù–ò–ö|–∏—Å—Ç–æ—á–Ω–∏–∫|–ò—Å—Ç–æ—á–Ω–∏–∫|Source|SOURCE|–ö–û–ù–¢–ï–ö–°–¢)\s+(\d+)'

    def replace_with_ref(match):
        source_num = match.group(2)
        return f"{match.group(1)} \\hyperlink{{row:{section_label}-{source_num}}}{{{source_num}}}"

    return re.sub(pattern, replace_with_ref, text)

jinja_env = jinja2.Environment(
    block_start_string='<BLOCK>',
    block_end_string='</BLOCK>',
    variable_start_string='<VAR>',
    variable_end_string='</VAR>',
    comment_start_string='<!--',
    comment_end_string='-->'
)

jinja_env.filters['filter_newlines'] = filter_newlines
jinja_env.filters['process_summary'] = process_summary

class Source(NamedTuple):
    source_label: str
    source_chunk: str
    source_filepath: str


class Section(NamedTuple):
    name: str
    label: str
    summary: str
    sources: list[Source]


class Status(StrEnum):
    confirmed = "confirmed"
    not_found = "not_found"
    partial = "partial"
    indirect = "indirect"
    requires_confirmation = "requires_confirmation"

    def get_ru_name(self) -> str:
        match self:
            case Status.confirmed:
                return "–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ"
            case Status.not_found:
                return "–ù–µ –Ω–∞–π–¥–µ–Ω–æ"
            case Status.partial:
                return "–ß–∞—Å—Ç–∏—á–Ω–æ –Ω–∞–π–¥–µ–Ω–æ"
            case Status.indirect:
                return "–ò–Ω–¥–∏—Ä–µ–∫—Ç–Ω–æ"
            case Status.requires_confirmation:
                return "–¢—Ä–µ–±—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"
            case _:
                raise NotImplementedError


CHAPTER_TEMPLATE = jinja_env.from_string("""
\\chapter{<VAR>chapter_name</VAR>}
\\label{cha:<VAR>chapter_label</VAR>}

<BLOCK>for section in sections</BLOCK>
\\section{<VAR>section.name</VAR>}
\\label{sec:<VAR>section.label</VAR>}

\\subsection{–ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞}
\\textbf{–°—Ç–∞—Ç—É—Å}: <VAR>chapter_name</VAR>. <VAR>section.summary | filter_newlines | process_summary(section.label)</VAR>

\\subsection{–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã}
\\begin{longtable}{|p{0.05\\textwidth}|p{0.65\\textwidth}|p{0.2\\textwidth}|}
\\hline
\\textbf{‚Ññ} & \\textbf{–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã} & \\textbf{–ò—Å—Ç–æ—á–Ω–∏–∫} \\\\
\\hline
\\endhead
<BLOCK>for source in section.sources</BLOCK>
<BLOCK>set source_index = loop.index</BLOCK>
\\raisebox{-\\baselineskip}[0pt][0pt]{\\hypertarget{row:<VAR>section.label</VAR>-<VAR>source_index</VAR>}{}} <VAR>source_index</VAR> & <VAR>source.source_chunk | filter_newlines</VAR> & \\cite{<VAR>source.source_label</VAR>} \\\\
\\hline
<BLOCK>endfor</BLOCK>
\\end{longtable}
<BLOCK>endfor</BLOCK>
""")

BIBLIOGRAPHY_TEMPLATE = jinja_env.from_string("""@misc{<VAR>source_label</VAR>,
    author = {–ü–ê–û ``–ì–∞–∑–ø—Ä–æ–º``},
    title = {<VAR>filename | filter_newlines</VAR>},
    howpublished = {–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç},
    year = {2025},
    note = {–°—Ç—Ä–∞–Ω–∏—Ü–∞: <VAR>page</VAR>, –°–ª–∞–π–¥: <VAR>slide</VAR>}
}
""")

LATEX_ROOT_PATH = Path(__file__).parent / "latex-gost-template"
LATEX_OUTPUT_PATH = LATEX_ROOT_PATH / "thesis.pdf"
LATEX_COMPILE_SCRIPT = LATEX_ROOT_PATH / "build.sh"
LATEX_TEMPLATE_PATH = LATEX_ROOT_PATH / "tex"
LATEX_SOURCE_PATH = LATEX_ROOT_PATH / "tex_tmp"


def create_safe_label(text: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ª–∞–±–µ–ª –¥–ª—è LaTeX –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    return text.lower().replace(' ', '-').replace(',', '').replace('.', '').replace('(', '').replace(')', '')


def create_source_label(filename: str, index: int) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    base_name = Path(filename).stem.lower().replace(' ', '-').replace('.', '-')
    return f"{base_name}-{index}"


def render_latex(file_path: Path, **kwargs) -> None:
    template = jinja_env.from_string(file_path.read_text())
    rendered = template.render(**kwargs)
    file_path.write_text(rendered)


def make_pdf(input_json: dict, output_pdf: Path) -> None:
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
    LATEX_SOURCE_PATH.mkdir(exist_ok=True)

    chapters = {status: [] for status in Status}
    json_data = input_json

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏–∏
    all_sources = []
    source_counter = {}

    for section_name, content in json_data.items():
        status = Status(content["status"])

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —ç—Ç–æ–π —Å–µ–∫—Ü–∏–∏
        section_sources = []
        for i, source_data in enumerate(content["sources"]):
            filename = source_data["filename"]
            source_counter[filename] = source_counter.get(filename, 0) + 1
            source_label = create_source_label(filename, source_counter[filename])

            section_sources.append(Source(
                source_label=source_label,
                source_chunk=source_data["snippet"],
                source_filepath=filename
            ))

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            all_sources.append({
                'source_label': source_label,
                'filename': filename,
                'page': source_data.get("page", "N/A") or "N/A",
                'slide': source_data.get("slide", "N/A") or "N/A"
            })

        # –°–æ–∑–¥–∞–µ–º —Å–µ–∫—Ü–∏—é
        section_label = create_safe_label(section_name)
        section = Section(
            name=section_name,
            label=section_label,
            summary=content["answer"],
            sources=section_sources
        )

        chapters[status].append(section)

    shutil.rmtree(LATEX_SOURCE_PATH)
    shutil.copytree(LATEX_TEMPLATE_PATH, LATEX_SOURCE_PATH)

    # –°–æ–∑–¥–∞–µ–º –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—é
    bib_content = []
    for source in all_sources:
        bib_entry = BIBLIOGRAPHY_TEMPLATE.render(**source)
        bib_content.append(bib_entry)

    bib_path = LATEX_SOURCE_PATH / "0-main.bib"
    bib_path.write_text('\n'.join(bib_content), encoding='utf-8')

    # –°–æ–∑–¥–∞–µ–º –≥–ª–∞–≤—ã
    chapter_ids = []
    for i, status in enumerate(Status, start=1):
        if not chapters[status]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –≥–ª–∞–≤—ã
            continue

        chapter_id = f"chapter-{i}"
        chapter_filename = f"3{i}-{chapter_id}.tex"
        chapter_path = LATEX_SOURCE_PATH / chapter_filename

        rendered_template = CHAPTER_TEMPLATE.render(
            chapter_name=status.get_ru_name(),
            chapter_label=f"chapter-{i}",
            sections=chapters[status]
        )

        chapter_path.write_text(rendered_template, encoding='utf-8')
        chapter_ids.append(chapter_filename.replace('.tex', ''))

    render_latex(LATEX_SOURCE_PATH / "0-main.tex", chapters=chapter_ids)
    render_latex(LATEX_SOURCE_PATH / "11-title-page.tex", doc_name="–°–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∑–∞–º–µ—á–∞–Ω–∏—è–º –∏ –ø—Ä–æ–≥—Ä–∞–º–º–µ –¥–æ–∏–∑—É—á–µ–Ω–∏—è",
                 doc_id=" GAZ-REP-2025-001", assigned_to="–û—Ç–¥–µ–ª –∞–Ω–∞–ª–∏—Ç–∏–∫–∏")
    render_latex(LATEX_SOURCE_PATH / "2-intro.tex", intro_text="""–ù–∞—Å—Ç–æ—è—â–∏–π –æ—Ç—á—ë—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö
–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω—ã –∫–∞–∫
–≥–ª–∞–≤—ã, –≥—Ä—É–ø–ø—ã ‚Äî –∫–∞–∫ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∞
–ø—Ä–∏–≤–µ–¥–µ–Ω—ã —Å–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∏—Å—Ö–æ–¥–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è""")
    render_latex(LATEX_SOURCE_PATH / "4-conclusion.tex", conclusion_text="""–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ —Å–Ω–∏–∂–µ–Ω–∏–µ
–Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–µ–π –∏ –ø–æ–≤—ã—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤.
–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –ø–ª–∞–Ω –¥–æ–∏–∑—É—á–µ–Ω–∏—è –∏
–∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ –∏—Ç–æ–≥–∞–º –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.""")

    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º LaTeX
    try:
        subprocess.run(['bash', str(LATEX_COMPILE_SCRIPT)], cwd=LATEX_SOURCE_PATH, check=True)

    except subprocess.CalledProcessError as e:
        print(f"–û—à–∏–±–∫–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ LaTeX: {e}")

    if LATEX_OUTPUT_PATH.exists():
        shutil.copy2(LATEX_OUTPUT_PATH, output_pdf)
        print(f"PDF —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {output_pdf}")
    else:
        print("–û—à–∏–±–∫–∞: PDF —Ñ–∞–π–ª –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω")

# --- RAG ---

class ComprehensiveRAGSystem:
    def __init__(self, config):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device} –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ '{self.config.get('PROJECT_NAME', 'Unknown')}'")

        print("–ó–∞–≥—Ä—É–∑–∫–∞ Embedding –º–æ–¥–µ–ª–∏...")
        self.embedding_model = HuggingFaceEmbeddings(
            model_name=self.config["EMBEDDING_MODEL"],
            model_kwargs={'device': self.device},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

        self.retriever = self._build_or_load_retriever()

    def _extract_text_from_pptx_safe(self, filepath, filename):
        docs = []
        try:
            prs = Presentation(filepath)
            for i, slide in enumerate(prs.slides):
                slide_texts = [
                    shape.text for shape in slide.shapes
                    if hasattr(shape, "text") and shape.text
                ]
                if slide_texts:
                    slide_content = "\n".join(slide_texts)
                    docs.append(Document(
                        page_content=slide_content,
                        metadata={"filename": filename, "slide": i + 1}
                    ))
            return docs
        except Exception as e:
            print(f"    ‚ö† PPTX (SVG –∏–ª–∏ –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –æ–±—ä–µ–∫—Ç) ‚Äì fallback —á–µ—Ä–µ–∑ PyMuPDF: {e}")
            try:
                with fitz.open(filepath) as ppt_as_pdf:
                    for page_num, page in enumerate(ppt_as_pdf):
                        page_text = page.get_text("text")
                        if page_text:
                            docs.append(Document(
                                page_content=page_text,
                                metadata={"filename": filename, "page": page_num + 1}
                            ))
            except Exception as inner_e:
                print(f"    –ù–µ —É–¥–∞–ª–æ—Å—å fallback-—Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å {filename} —á–µ—Ä–µ–∑ PyMuPDF: {inner_e}")
            return docs

    def _extract_text_from_docs(self, folder_path):
        if not os.path.exists(folder_path):
            print(f"    –ü–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
            return []
        all_docs = []
        print(f"--- –ù–∞—á–∞–ª–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä) –∏–∑ '{folder_path}' ---")

        for filename in os.listdir(folder_path):
            full_path = os.path.join(folder_path, filename)
            try:
                if filename.endswith(".docx"):
                    doc = docx.Document(full_path)
                    content_parts = []
                    for para in doc.paragraphs:
                        if para.text.strip(): content_parts.append(para.text)
                    for table in doc.tables:
                        for row in table.rows:
                            row_text = " | ".join([cell.text.strip() for cell in row.cells])
                            if row_text: content_parts.append(row_text)
                    full_text = "\n".join(content_parts)
                    if full_text: all_docs.append(Document(page_content=full_text, metadata={"filename": filename}))

                elif filename.endswith(".pdf"):
                    with fitz.open(full_path) as pdf_doc:
                        for page_num, page in enumerate(pdf_doc):
                            page_text = page.get_text("text")
                            if page_text: all_docs.append(
                                Document(page_content=page_text, metadata={"filename": filename, "page": page_num + 1}))

                elif filename.endswith(".pptx"):
                    docs_from_pptx = self._extract_text_from_pptx_safe(full_path, filename)
                    all_docs.extend(docs_from_pptx)

                elif filename.endswith((".html", ".htm")):
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                        html = f.read()
                    soup = BeautifulSoup(html, "lxml")
                    for tag in soup(["script", "style"]): tag.decompose()
                    text = soup.get_text(" ", strip=True)
                    if text: all_docs.append(Document(page_content=text, metadata={"filename": filename}))

                elif filename.endswith(".txt"):
                    with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                        text = f.read()
                    if text: all_docs.append(Document(page_content=text, metadata={"filename": filename}))

                print(f"     –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª: {filename}")
            except Exception as e:
                print(f"     –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {filename}: {e}")
        return all_docs

    def _split_text_into_chunks(self, documents):
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
        return text_splitter.split_documents(documents)

    def _build_or_load_retriever(self):
        index_path, chunks_path = self.config["INDEX_FILE_PATH"], self.config["CHUNKS_FILE_PATH"]
        index_dir = os.path.dirname(index_path)
        index_name = os.path.basename(index_path).replace('.faiss', '')

        if os.path.exists(index_path):
            print(f"--- –ù–∞–π–¥–µ–Ω —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –¥–ª—è '{self.config['PROJECT_NAME']}'. –ó–∞–≥—Ä—É–∂–∞–µ–º... ---")
            faiss_store = FAISS.load_local(
                folder_path=index_dir, embeddings=self.embedding_model, index_name=index_name,
                allow_dangerous_deserialization=True
            )
            with open(chunks_path, 'r', encoding='utf-8') as f:
                chunks_json = json.load(f)
            split_docs = [Document(page_content=c["page_content"], metadata=c["metadata"]) for c in chunks_json]
        else:
            print(f"--- –ò–Ω–¥–µ–∫—Å –¥–ª—è '{self.config['PROJECT_NAME']}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π... ---")
            documents = self._extract_text_from_docs(self.config["DOCUMENTS_PATH"])
            if not documents: raise FileNotFoundError(
                f"–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ '{self.config['PROJECT_NAME']}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {self.config['DOCUMENTS_PATH']}.")
            split_docs = self._split_text_into_chunks(documents)
            print(f"\n--- –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º {len(split_docs)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ '{self.config['PROJECT_NAME']}'... ---")
            faiss_store = FAISS.from_documents(split_docs, self.embedding_model)
            faiss_store.save_local(folder_path=index_dir, index_name=index_name)
            chunks_for_json = [{"page_content": doc.page_content, "metadata": doc.metadata} for doc in split_docs]
            with open(chunks_path, 'w', encoding='utf-8') as f:
                json.dump(chunks_for_json, f, ensure_ascii=False, indent=2)

        faiss_retriever = faiss_store.as_retriever(search_kwargs={"k": self.config["RETRIEVER_TOP_K"]})
        bm25_retriever = BM25Retriever.from_documents(split_docs)
        bm25_retriever.k = self.config["RETRIEVER_TOP_K"]

        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
        )
        print(f"      –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä (BM25 + FAISS) –≥–æ—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ '{self.config['PROJECT_NAME']}'.")
        return ensemble_retriever

    # --- –ò–ó–ú–ï–ù–ï–ù–û: –î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä "format": "json" –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ JSON ---
    async def _call_local_llm(self, messages):
        payload = {
            "model": self.config["LOCAL_MODEL_NAME"],
            "messages": messages,
            "stream": False,
            "format": "json",
            "options": {
                "temperature": 0.2,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                "top_p": 0.9,  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è  –≤—ã–±–æ—Ä–∫–∞
                "repetition_penalty": 1.05  #  —à—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã
            }
        }
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.config["LOCAL_API_URL"], json=payload, timeout=300) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        print(f"  [LLM SERVER ERROR] –°—Ç–∞—Ç—É—Å {response.status}. –û—Ç–≤–µ—Ç: {error_text[:200]}")
                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º JSON —Å –æ—à–∏–±–∫–æ–π, —á—Ç–æ–±—ã –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –º–æ–≥ —ç—Ç–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å
                        return json.dumps({"status": "requires_confirmation", "answer": f"–û—à–∏–±–∫–∞ API: {error_text}"})
                    response_data = await response.json()
                    # –û—Ç–≤–µ—Ç –æ—Ç Ollama –≤ JSON mode –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ message.content –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
                    return response_data.get('message', {}).get('content', '')
        except Exception as e:
            print(f"  [LLM CONNECTION EXCEPTION] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {type(e).__name__}: {e}")
            return json.dumps({"status": "requires_confirmation", "answer": f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API: {e}"})

    async def _expand_query(self, criterion):
        prompt = f"""–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å —Ç—Ä–µ–º—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–π —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –º–µ–Ω—è–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É. –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ 3 –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏, –∫–∞–∂–¥–∞—è –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï.

–ò–°–•–û–î–ù–´–ô –ó–ê–ü–†–û–°: "{criterion}"

–ü–ï–†–ï–§–û–†–ú–£–õ–ò–†–û–í–ê–ù–ù–´–ï –ó–ê–ü–†–û–°–´:"""
        response_text = await self._call_local_llm([{"role": "user", "content": prompt}])
        if "–û—à–∏–±–∫–∞" in response_text: return [criterion]
        expanded = [q.strip().lstrip("-* ").strip() for q in response_text.split('\n') if q.strip()]
        print(f"      ... –∑–∞–ø—Ä–æ—Å '{criterion[:30]}...' —Ä–∞—Å—à–∏—Ä–µ–Ω –¥–æ: {expanded}")
        return [criterion] + expanded



    async def process_criterion(self, criterion):
        print(f"\n--- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏—è –¥–ª—è '{self.config['PROJECT_NAME']}': '{criterion[:70]}...' ---")

        all_queries = await self._expand_query(criterion)
        tasks = [asyncio.to_thread(self.retriever.invoke, q) for q in all_queries]
        results_from_queries = await asyncio.gather(*tasks)

        unique_docs = {doc.page_content: doc for doc_list in results_from_queries for doc in doc_list}
        retrieved_docs = list(unique_docs.values())
        print(f"    üîç –ù–∞–π–¥–µ–Ω–æ {len(retrieved_docs)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤.")

        if not retrieved_docs:
            return {"answer": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.", "status": "not_found", "sources": []}

        final_docs = sorted(retrieved_docs, key=lambda x: x.metadata.get('score', 0), reverse=True)[
                     :self.config["RETRIEVER_TOP_K"]]
        context = ""
        for i, doc in enumerate(final_docs):
            source_info = f"[–ò–°–¢–û–ß–ù–ò–ö {i + 1}: {doc.metadata.get('filename', 'N/A')}, —Å—Ç—Ä. {doc.metadata.get('page', 'N/A')}, —Å–ª–∞–π–¥ {doc.metadata.get('slide', 'N/A')}]"
            context += f"{source_info}\n{doc.page_content}\n\n"

        # --- –ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç, –∑–∞–ø—Ä–∞—à–∏–≤–∞—é—â–∏–π JSON ---
        final_prompt = f"""–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç—ã —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –ö–û–ù–¢–ï–ö–°–¢ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –í–û–ü–†–û–° –ù–ê –†–£–°–°–ö–û–ú.

–ö–û–ù–¢–ï–ö–°–¢:
---
{context.strip()}
---

–í–û–ü–†–û–°: "{criterion}"

–¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¢–û–õ–¨–ö–û JSON –æ–±—ä–µ–∫—Ç–æ–º —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π:
{{
  "status": "–û–î–ò–ù –ò–ó –°–¢–ê–¢–£–°–û–í: confirmed, not_found, partial, indirect, requires_confirmation",
  "answer": "–¢–≤–æ–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [–ò–°–¢–û–ß–ù–ò–ö N] –ù–ê –†–£–°–°–ö–û–ú"
}}
"""
        raw_json_string = await self._call_local_llm([{"role": "user", "content": final_prompt}])

        try:
            data = json.loads(raw_json_string)
            clean_answer = data.get("answer", "–ö–ª—é—á 'answer' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏.")
            status = data.get("status", "requires_confirmation")
            # –ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞
            valid_statuses = {"confirmed", "not_found", "partial", "indirect", "requires_confirmation"}
            if status not in valid_statuses:
                status = "requires_confirmation"
        except (json.JSONDecodeError, TypeError):
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –∏–ª–∏ –≤–æ–æ–±—â–µ –Ω–µ JSON
            clean_answer = "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON. –û—Ç–≤–µ—Ç: " + str(raw_json_string)
            status = "requires_confirmation"

        sources = [
            {"filename": d.metadata.get("filename"), "page": d.metadata.get("page"), "slide": d.metadata.get("slide"),
             "snippet": d.page_content} for d in final_docs]

        return {"answer": clean_answer, "status": status, "sources": sources}


def parse_checklist_from_csv(filename):
    try:
        if not os.path.exists(filename):
            print(f"     –§–∞–π–ª —á–µ–∫-–ª–∏—Å—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {filename}")
            return []
        df = pd.read_csv(filename)
        if 'criterion' not in df.columns:
            print(f"     –í —Ñ–∞–π–ª–µ '{filename}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'criterion'.")
            return []
        criteria = df['criterion'].dropna().astype(str).tolist()
        print(f"    –ù–∞–π–¥–µ–Ω–æ {len(criteria)} –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –≤ CSV —Ñ–∞–π–ª–µ '{filename}'.")
        return criteria
    except Exception as e:
        print(f"    –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ CSV —Ñ–∞–π–ª–∞ '{filename}': {e}")
        return []


async def main(project_names):
    for project_name in project_names:
        print(f"\n======== –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–†–ê–ë–û–¢–ö–£ –ü–†–û–ï–ö–¢–ê: {project_name} ========")

        project_folder = os.path.join(".", project_name)
        documents_path = os.path.join(project_folder, "documents")
        checklist_file = os.path.join(project_folder, f"checklist_{project_name.lower()}.csv")
        index_file_path = os.path.join(project_folder, "vector_index.faiss")
        chunks_file_path = os.path.join(project_folder, "chunks_meta.json")
        report_path = os.path.join(project_folder,
                                   f"verification_report_{project_name}_RAG_FINAL.pdf")

        current_project_config = GLOBAL_CONFIG.copy()
        current_project_config.update({
            "PROJECT_NAME": project_name,
            "PROJECT_FOLDER": project_folder,
            "DOCUMENTS_PATH": documents_path,
            "CHECKLIST_FILE": checklist_file,
            "INDEX_FILE_PATH": index_file_path,
            "CHUNKS_FILE_PATH": chunks_file_path,
        })

        if not os.path.exists(current_project_config["PROJECT_FOLDER"]):
            os.makedirs(current_project_config["PROJECT_FOLDER"])
            os.makedirs(current_project_config["DOCUMENTS_PATH"])
            print(f"–°–æ–∑–¥–∞–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞: {project_name}")

        try:
            system = ComprehensiveRAGSystem(current_project_config)

            print(
                f"\n--- –≠–¢–ê–ü 2: –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ —á–µ–∫-–ª–∏—Å—Ç—É '{current_project_config['CHECKLIST_FILE']}' –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ '{project_name}' ---")
            criteria_to_check = parse_checklist_from_csv(current_project_config['CHECKLIST_FILE'])
            if not criteria_to_check:
                print(f"    –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–µ–∫—Ç {project_name}: –ù–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.")
                continue

            semaphore = asyncio.Semaphore(current_project_config["MAX_CONCURRENT_REQUESTS"])

            async def process_with_semaphore(criterion):
                async with semaphore:
                    result = await system.process_criterion(criterion)
                    await asyncio.sleep(current_project_config["REQUEST_DELAY_SECONDS"])
                    return result

            tasks = [process_with_semaphore(c) for c in criteria_to_check]
            results = await async_tqdm.gather(*tasks, desc=f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è {project_name}")
            final_report = {c: r for c, r in zip(criteria_to_check, results)}

            print(f"\n--- –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ –í–ï–†–ò–§–ò–ö–ê–¶–ò–ò –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ {project_name} ---")

            make_pdf(final_report, Path(report_path))

            print(f"\n –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}")

        except FileNotFoundError as e:
            print(f"     –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–µ–∫—Ç–∞ {project_name}: {e}")
        except Exception as e:
            print(f"     –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–µ–∫—Ç–∞ {project_name}: {type(e).__name__}: {e}")

        print(f"\n======== –ó–ê–í–ï–†–®–ï–ù–ê –û–ë–†–ê–ë–û–¢–ö–ê –ü–†–û–ï–ö–¢–ê: {project_name} ========\n")


if __name__ == "__main__":
    projects_to_process = ["Project_Alfa"] # –û—Å—Ç–∞–≤–∏–ª –æ–¥–∏–Ω –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
    asyncio.run(main(projects_to_process))